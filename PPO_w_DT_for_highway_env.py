import gymnasium as gym
import highway_env
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import matplotlib.pyplot as plt
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv
import pickle
import os
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass
import json
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

@dataclass
class TrajectoryData:
   
    states: np.ndarray
    actions: np.ndarray
    rewards: np.ndarray
    returns: np.ndarray
    timesteps: np.ndarray

class FixedObsWrapper(gym.ObservationWrapper):
    
    def __init__(self, env):
        super().__init__(env)
        
  
        original_reset = env.reset()
        if isinstance(original_reset, tuple):
            original_obs = original_reset[0]  
        else:
            original_obs = original_reset
            
    
        if isinstance(original_obs, np.ndarray):
            if len(original_obs.shape) > 1:
                self.obs_flat = original_obs.flatten()
            else:
                self.obs_flat = original_obs
        else:
            self.obs_flat = np.array(original_obs).flatten()
        
        self.flattened_size = len(self.obs_flat)

        self.observation_space = gym.spaces.Box(
            low=-np.inf,
            high=np.inf,
            shape=(self.flattened_size,),
            dtype=np.float32
        )
    
    def observation(self, obs):

        if isinstance(obs, np.ndarray):
            obs_flat = obs.flatten()
        else:
            obs_flat = np.array(obs).flatten()
    
        if len(obs_flat) != self.flattened_size:
            if len(obs_flat) > self.flattened_size:
                obs_flat = obs_flat[:self.flattened_size]
            else:
                
                padded = np.zeros(self.flattened_size, dtype=np.float32)
                padded[:len(obs_flat)] = obs_flat
                obs_flat = padded
        
        return obs_flat.astype(np.float32)
    
    def reset(self, **kwargs):

        reset_result = self.env.reset(**kwargs)
        if isinstance(reset_result, tuple):
            obs, info = reset_result
            return self.observation(obs), info
        else:
            return self.observation(reset_result)
    
    def step(self, action):

        step_result = self.env.step(action)
        if len(step_result) == 4:
            obs, reward, done, info = step_result
            return self.observation(obs), reward, done, info
        else:  
            obs, reward, done, truncated, info = step_result
            return self.observation(obs), reward, done, truncated, info

class HighwayPPOTrainer:
    
    def __init__(self, style="normal"):
        self.style = style
        self.env_config = self._get_env_config(style)
        
    def _get_env_config(self, style):

        base_config = {
            "observation": {
                "type": "Kinematics",
                "vehicles_count": 5,
                "features": ["presence", "x", "y", "vx", "vy"],
                "features_range": {
                    "x": [-100, 100],
                    "y": [-100, 100],
                    "vx": [-20, 20],
                    "vy": [-20, 20]
                },
                "absolute": False,
                "flatten": True,
                "observe_intentions": False
            },
            "action": {
                "type": "DiscreteMetaAction",
            },
            "lanes_count": 4,
            "vehicles_count": 30,
            "duration": 40,
            "initial_spacing": 2,
            "collision_reward": -1,
            "simulation_frequency": 15,
            "policy_frequency": 1,
            "other_vehicles_type": "highway_env.vehicle.behavior.IDMVehicle",
            "screen_width": 600,
            "screen_height": 150,
            "centering_position": [0.3, 0.5],
            "scaling": 5.5,
            "show_trajectories": False,
            "render_agent": True,
            "offscreen_rendering": False
        }
        
        if style == "aggressive":
            base_config.update({
                "reward_speed_range": [20, 30],
                "high_speed_reward": 0.5,
                "collision_reward": -0.5,
                "right_lane_reward": 0.1,
                "lane_change_reward": 0.2
            })
        elif style == "conservative":
            base_config.update({
                "reward_speed_range": [15, 25],
                "high_speed_reward": 0.2,
                "collision_reward": -2.0,
                "right_lane_reward": 0.3,
                "lane_change_reward": -0.1
            })
        else:  # normal
            base_config.update({
                "reward_speed_range": [20, 30],
                "high_speed_reward": 0.3,
                "collision_reward": -1.0,
                "right_lane_reward": 0.2,
                "lane_change_reward": 0.0
            })
            
        return base_config
    
    def create_env(self):

        def _init():
            env = gym.make('highway-fast-v0')
            env.configure(self.env_config)
            wrapped_env = FixedObsWrapper(env)
            return wrapped_env

        return DummyVecEnv([_init])
    
    def train_ppo(self, total_timesteps=20000, save_path=None):

        env = self.create_env()
        
        test_obs = env.reset()
        actual_obs_size = test_obs.shape[1] if len(test_obs.shape) > 1 else len(test_obs)

        policy_kwargs = dict(net_arch=[256, 256])
        model = PPO(
            "MlpPolicy", 
            env, 
            learning_rate=0.0003,
            n_steps=1024,
            batch_size=32,
            ent_coef=0.01,
            policy_kwargs=policy_kwargs,
            verbose=1,
            device=device
        )
        
        print(f"Training {self.style} style PPO model...")
        try:
            model.learn(total_timesteps=total_timesteps)
        except Exception as e:
            model = PPO(
                "MlpPolicy",
                env,
                learning_rate=0.0003,
                n_steps=512,
                batch_size=16,
                verbose=1,
                device=device
            )
            model.learn(total_timesteps=total_timesteps//2)
        
        if save_path:
            os.makedirs(os.path.dirname(save_path), exist_ok=True)
            model.save(save_path)
            print(f"Model saved to {save_path}")
        
        env.close()
        return model
    
    def collect_trajectories(self, model, num_episodes=20, save_path=None):

        env = gym.make('highway-fast-v0')
        env.configure(self.env_config)
        env = FixedObsWrapper(env)
        
        trajectories = []
        
        for episode in range(num_episodes):
            trajectory = {
                'states': [],
                'actions': [],
                'rewards': [],
                'dones': []
            }
            

            reset_result = env.reset()
            if isinstance(reset_result, tuple):
                obs, info = reset_result
            else:
                obs = reset_result
                
            done = False
            step = 0
            
            while not done and step < 300:
                try:

                    if len(obs.shape) == 1:
                        obs_for_prediction = obs.reshape(1, -1)
                    else:
                        obs_for_prediction = obs
                    
                    action, _ = model.predict(obs_for_prediction, deterministic=False)
                    
                    if isinstance(action, np.ndarray):
                        action = action[0] if action.shape == (1,) else action.item()
                    
                    step_result = env.step(action)
                    if len(step_result) == 4:
                        next_obs, reward, done, info = step_result
                    else:
                        next_obs, reward, done, truncated, info = step_result
                        done = done or truncated
                    
                    trajectory['states'].append(obs.copy())
                    trajectory['actions'].append(action)
                    trajectory['rewards'].append(reward)
                    trajectory['dones'].append(done)
                    
                    obs = next_obs
                    step += 1
                    
                except Exception as e:
                    print(f"Error!: {e}")
                    break
            
            if len(trajectory['states']) > 5:  
                # Calculate return-to-go
                rewards = np.array(trajectory['rewards'])
                returns = np.zeros_like(rewards, dtype=np.float32)
                returns[-1] = rewards[-1]
                for i in range(len(rewards) - 2, -1, -1):
                    returns[i] = rewards[i] + 0.99 * returns[i + 1]
                
                trajectory['returns'] = returns
                trajectory['timesteps'] = np.arange(len(rewards))
                
                trajectories.append(trajectory)
            
            if (episode + 1) % 10 == 0:
                print(f"Collected {episode + 1}/{num_episodes} episodes, valid trajectories: {len(trajectories)}")
        
        env.close()
        
        if save_path:
            os.makedirs(os.path.dirname(save_path), exist_ok=True)
            with open(save_path, 'wb') as f:
                pickle.dump(trajectories, f)
            print(f"Trajectories saved to {save_path}")
        
        return trajectories

class HighwayTrajectoryDataset(Dataset):
    
    def __init__(self, trajectories, context_length=20):
        self.trajectories = trajectories
        self.context_length = context_length
        self.data = self._prepare_data()
    
    def _prepare_data(self):
        data = []
        
        for traj_idx, traj in enumerate(self.trajectories):
            try:
                states = np.array(traj['states'])
                actions = np.array(traj['actions'])
                returns = np.array(traj['returns'])
                timesteps = np.array(traj['timesteps'])
                
                if len(states.shape) == 1:
                    continue 
                
                traj_length = min(len(states), len(actions), len(returns), len(timesteps))
                
                if traj_length < 3:  
                    continue
                
                for i in range(max(0, traj_length - self.context_length + 1)):
                    end_idx = min(i + self.context_length, traj_length)
                    seq_len = end_idx - i
                    
                    if seq_len >= 3:  
                        seq_states = states[i:end_idx]
                        seq_actions = actions[i:end_idx]
                        seq_returns = returns[i:end_idx]
                        seq_timesteps = timesteps[i:end_idx]
                        
                        if seq_len < self.context_length:
                            pad_len = self.context_length - seq_len
                            state_dim = seq_states.shape[1]
                            
                            state_pad = np.zeros((pad_len, state_dim))
                            action_pad = np.zeros(pad_len)
                            return_pad = np.zeros(pad_len)
                            timestep_pad = np.arange(seq_len, seq_len + pad_len)
                            
                            seq_states = np.concatenate([seq_states, state_pad], axis=0)
                            seq_actions = np.concatenate([seq_actions, action_pad])
                            seq_returns = np.concatenate([seq_returns, return_pad])
                            seq_timesteps = np.concatenate([seq_timesteps, timestep_pad])
                        
                        data.append({
                            'states': seq_states,
                            'actions': seq_actions,
                            'returns': seq_returns,
                            'timesteps': seq_timesteps
                        })
                        
            except Exception as e:
                print(f"{traj_idx}: {e}")
                continue
        
        print(f"{len(data)}")
        return data
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        item = self.data[idx]
        return {
            'states': torch.FloatTensor(item['states']),
            'actions': torch.LongTensor(item['actions']),
            'returns': torch.FloatTensor(item['returns']),
            'timesteps': torch.LongTensor(item['timesteps'])
        }

class DecisionTransformer(nn.Module):
    
    def __init__(self, state_dim, action_dim, context_length=20, 
                 embed_dim=128, n_layer=3, n_head=1, dropout=0.1):
        super().__init__()
        
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.context_length = context_length
        self.embed_dim = embed_dim
        
        self.state_embed = nn.Linear(state_dim, embed_dim)
        self.action_embed = nn.Embedding(action_dim, embed_dim)
        self.return_embed = nn.Linear(1, embed_dim)
        self.timestep_embed = nn.Embedding(1000, embed_dim)
        
        # Layer normalization
        self.embed_ln = nn.LayerNorm(embed_dim)
        
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=embed_dim,
            nhead=n_head,
            dim_feedforward=4*embed_dim,
            dropout=dropout,
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layer)
        
        self.action_head = nn.Linear(embed_dim, action_dim)
        
        self.pos_embed = nn.Parameter(torch.zeros(1, 3*context_length, embed_dim))
        
    def forward(self, states, actions, returns, timesteps, attention_mask=None):
        try:
            batch_size, seq_len = states.shape[0], states.shape[1]
            
            if len(states.shape) == 3 and states.shape[2] == self.state_dim:
                state_embeds = self.state_embed(states)
            else:
                states_reshaped = states.view(batch_size, seq_len, -1)
                if states_reshaped.shape[2] != self.state_dim:
                    if states_reshaped.shape[2] > self.state_dim:
                        states_reshaped = states_reshaped[:, :, :self.state_dim]
                    else:
                        pad_size = self.state_dim - states_reshaped.shape[2]
                        padding = torch.zeros(batch_size, seq_len, pad_size, device=states.device)
                        states_reshaped = torch.cat([states_reshaped, padding], dim=2)
                state_embeds = self.state_embed(states_reshaped)
            
            if len(actions.shape) == 2:
                actions = actions.long()
            else:
                actions = actions.view(batch_size, seq_len).long()
            
            actions = torch.clamp(actions, 0, self.action_dim - 1)
            
            action_embeds = self.action_embed(actions)
            return_embeds = self.return_embed(returns.unsqueeze(-1) if len(returns.shape) == 2 else returns)
            
            timesteps_clamped = torch.clamp(timesteps, 0, 999)
            timestep_embeds = self.timestep_embed(timesteps_clamped)
            
            sequence = torch.zeros((batch_size, 3*seq_len, self.embed_dim), 
                                 device=states.device, dtype=state_embeds.dtype)
            
            for i in range(seq_len):
                sequence[:, 3*i, :] = return_embeds[:, i, :]
                sequence[:, 3*i+1, :] = state_embeds[:, i, :]
                sequence[:, 3*i+2, :] = action_embeds[:, i, :]
            
            if hasattr(self, 'pos_embed') and self.pos_embed.shape[1] >= 3*seq_len:
                pos_embed_truncated = self.pos_embed[:, :3*seq_len, :]
                sequence = sequence + pos_embed_truncated
            
            sequence = self.embed_ln(sequence)
            
            if attention_mask is not None:
                mask = torch.triu(torch.ones(3*seq_len, 3*seq_len), diagonal=1).bool()
                mask = mask.to(states.device)
            else:
                mask = None
                
            output = self.transformer(sequence, mask=mask)
            action_outputs = output[:, 1::3, :] 
            action_logits = self.action_head(action_outputs)
            
            return action_logits
            
        except Exception as e:
            print(f"Error in DecisionTransformer forward: {e}")
    
            batch_size, seq_len = states.shape[0], states.shape[1]
            return torch.zeros(batch_size, seq_len, self.action_dim, device=states.device)

class DecisionTransformerTrainer:
    """Decision Transformer"""
    
    def __init__(self, model, device, lr=1e-4):
        self.model = model.to(device)
        self.device = device
        self.optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)
        self.criterion = nn.CrossEntropyLoss()
        
    def train_step(self, batch):
       
        states = batch['states'].to(self.device)
        actions = batch['actions'].to(self.device)
        returns = batch['returns'].to(self.device)
        timesteps = batch['timesteps'].to(self.device)
        
        action_logits = self.model(states, actions, returns, timesteps)
        
        if action_logits.shape[1] > 1:
            targets = actions[:, 1:].contiguous().view(-1)
            predictions = action_logits[:, :-1].contiguous().view(-1, action_logits.size(-1))
        else:
            targets = actions.contiguous().view(-1)
            predictions = action_logits.contiguous().view(-1, action_logits.size(-1))
        
        valid_mask = targets >= 0
        if valid_mask.sum() > 0:
            targets = targets[valid_mask]
            predictions = predictions[valid_mask]
        
        if len(targets) > 0:
            loss = self.criterion(predictions, targets)
        else:
            loss = torch.tensor(0.0, device=self.device, requires_grad=True)
        
        self.optimizer.zero_grad()
        if loss.requires_grad and loss.item() > 0:
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.25)
            self.optimizer.step()
        
        return loss.item()
    
    def train(self, train_loader, epochs=50, val_loader=None):

        train_losses = []
        val_losses = []
        
        for epoch in range(epochs):
            self.model.train()
            epoch_losses = []
            
            for batch_idx, batch in enumerate(train_loader):
                try:
                    loss = self.train_step(batch)
                    epoch_losses.append(loss)
                    
                    if batch_idx % 10 == 0:
                        print(f"Epoch {epoch+1}, Batch {batch_idx}, Loss: {loss:.4f}")
                        
                except Exception as e:
                    print(f"Error in batch {batch_idx}: {e}")
                    continue
            
            if epoch_losses:
                avg_train_loss = np.mean(epoch_losses)
                train_losses.append(avg_train_loss)
                print(f"Epoch {epoch+1}/{epochs}: Average Train Loss: {avg_train_loss:.4f}")
            else:
                print(f"Epoch {epoch+1}/{epochs}: No valid batches processed")
                train_losses.append(float('nan'))
        
        return train_losses, val_losses

class HighwayEvaluator:
    
    def __init__(self, env_config):
        self.env_config = env_config
        
    def evaluate_model(self, model, is_dt=False, num_episodes=20, target_return=10.0):

        env = gym.make('highway-fast-v0')
        env.configure(self.env_config)
        env = FixedObsWrapper(env)
        
        metrics = {
            'total_rewards': [],
            'episode_lengths': [],
            'collision_rates': [],
            'avg_speeds': [],
            'lane_changes': []
        }
        
        for episode in range(num_episodes):
            try:
            
                reset_result = env.reset()
                if isinstance(reset_result, tuple):
                    obs, info = reset_result
                else:
                    obs = reset_result
                    
                done = False
                total_reward = 0
                steps = 0
                collisions = 0
                speeds = []
                lane_changes = 0
                last_lane = None

                if is_dt:
                    context_length = getattr(model, 'context_length', 10)
                    
                    states_history = [obs.copy()]
                    actions_history = [0]
                    returns_history = [target_return]
                    timesteps_history = [0]
                
                while not done and steps < 200:
                    try:
                        if is_dt:
            
                            action = self._predict_dt_action(
                                model, states_history, actions_history, 
                                returns_history, timesteps_history, 
                                context_length, target_return, total_reward, steps
                            )
                        else:
                        
                            if len(obs.shape) == 1:
                                obs_for_prediction = obs.reshape(1, -1)
                            else:
                                obs_for_prediction = obs
                            
                            action, _ = model.predict(obs_for_prediction, deterministic=True)
                            
                            if isinstance(action, np.ndarray):
                                action = action[0] if action.shape == (1,) else action.item()
                        
                        action = max(0, min(4, int(action)))
                        
                        step_result = env.step(action)
                        if len(step_result) == 4:
                            next_obs, reward, done, info = step_result
                        else:
                            next_obs, reward, done, truncated, info = step_result
                            done = done or truncated
                        
                        total_reward += reward
                        steps += 1
                        
                        # Crash
                        if reward < -0.5 or info.get('crashed', False):
                            collisions += 1
                        
                        # Velocity
                        try:
                            if len(obs) >= 5:
                                speed = np.sqrt(obs[3]**2 + obs[4]**2)
                                speeds.append(speed)
                        except:
                            speeds.append(0)
                        
                        # Lane change
                        try:
                            if len(obs) >= 3:
                                current_lane = obs[2]
                                if last_lane is not None and abs(current_lane - last_lane) > 1.0:
                                    lane_changes += 1
                                last_lane = current_lane
                        except:
                            pass
                        
                        # Updata Decision Transformer State
                        if is_dt:
                            states_history.append(next_obs.copy())
                            actions_history.append(action)
                            returns_history.append(target_return - total_reward)
                            timesteps_history.append(steps)
                            
                            if len(states_history) > context_length:
                                states_history = states_history[-context_length:]
                                actions_history = actions_history[-context_length:]
                                returns_history = returns_history[-context_length:]
                                timesteps_history = timesteps_history[-context_length:]
                        
                        obs = next_obs
                        
                    except Exception as e:
                        print(f"Episode {episode}, Step {steps} Âá∫Èîô: {e}")
                        break
                
                metrics['total_rewards'].append(total_reward)
                metrics['episode_lengths'].append(steps)
                metrics['collision_rates'].append(collisions / steps if steps > 0 else 0)
                metrics['avg_speeds'].append(np.mean(speeds) if speeds else 0)
                metrics['lane_changes'].append(lane_changes)
                
                if episode % 5 == 0:
                    print(f"Episode {episode}: Reward={total_reward:.2f}, Steps={steps}, Collisions={collisions}")
                    
            except Exception as e:
                print(f"Episode {episode} Error: {e}")
                metrics['total_rewards'].append(0)
                metrics['episode_lengths'].append(0)
                metrics['collision_rates'].append(1.0)
                metrics['avg_speeds'].append(0)
                metrics['lane_changes'].append(0)
                continue
        
        env.close()
        
        if len(metrics['total_rewards']) > 0:
            results = {
                'avg_reward': np.mean(metrics['total_rewards']),
                'std_reward': np.std(metrics['total_rewards']),
                'avg_episode_length': np.mean(metrics['episode_lengths']),
                'collision_rate': np.mean(metrics['collision_rates']),
                'avg_speed': np.mean(metrics['avg_speeds']),
                'avg_lane_changes': np.mean(metrics['lane_changes'])
            }
        else:
            results = {
                'avg_reward': 0.0,
                'std_reward': 0.0,
                'avg_episode_length': 0.0,
                'collision_rate': 1.0,
                'avg_speed': 0.0,
                'avg_lane_changes': 0.0
            }
        
        return results, metrics
    
    def _predict_dt_action(self, model, states_history, actions_history, 
                          returns_history, timesteps_history, context_length, 
                          target_return, total_reward, steps):
        try:
            seq_len = min(len(states_history), context_length)
            
            if seq_len < context_length:
                pad_len = context_length - seq_len
                state_dim = len(states_history[0])
                
                padded_states = [np.zeros(state_dim) for _ in range(pad_len)] + states_history
                padded_actions = [0] * pad_len + actions_history
                padded_returns = [0.0] * pad_len + returns_history
                padded_timesteps = list(range(-pad_len, 0)) + timesteps_history
            else:
                padded_states = states_history[-context_length:]
                padded_actions = actions_history[-context_length:]
                padded_returns = returns_history[-context_length:]
                padded_timesteps = timesteps_history[-context_length:]
            
            states_tensor = torch.FloatTensor(padded_states).unsqueeze(0).to(device)
            actions_tensor = torch.LongTensor(padded_actions).unsqueeze(0).to(device)
            returns_tensor = torch.FloatTensor(padded_returns).unsqueeze(0).to(device)
            timesteps_tensor = torch.LongTensor(padded_timesteps).unsqueeze(0).to(device)
            
            timesteps_tensor = torch.clamp(timesteps_tensor, 0, 999)
            
            with torch.no_grad():
                model.eval()
                action_logits = model(states_tensor, actions_tensor, 
                                    returns_tensor, timesteps_tensor)
                
                action = torch.argmax(action_logits[0, -1]).item()
                return action
                
        except Exception as e:
            print(f"Error in DT prediction: {e}")
    
            return np.random.randint(0, 5)

class HighwayVisualizer:
    
    def __init__(self, models_dict, env_config):
        self.models = models_dict
        self.env_config = env_config
        
    def plot_training_curves(self, train_losses, val_losses, save_path="results/training_curves.png"):
        plt.figure(figsize=(12, 5))
        
        plt.subplot(1, 2, 1)
        plt.plot(train_losses, label='Training Loss', linewidth=2)
        if val_losses:
            plt.plot(val_losses, label='Validation Loss', linewidth=2)
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.title('Decision Transformer Training Progress')
        plt.legend()
        plt.grid(True)
        
        plt.subplot(1, 2, 2)
        if len(train_losses) > 1:
            smoothed_losses = []
            window = min(5, len(train_losses))
            for i in range(len(train_losses)):
                start_idx = max(0, i - window + 1)
                smoothed_losses.append(np.mean(train_losses[start_idx:i+1]))
            plt.plot(smoothed_losses, label='Smoothed Loss', linewidth=2, color='red')
            plt.xlabel('Epoch')
            plt.ylabel('Smoothed Loss')
            plt.title('Training Loss (Smoothed)')
            plt.legend()
            plt.grid(True)
        
        plt.tight_layout()
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"Results saved to {save_path}")

    def plot_evaluation_comparison(self, evaluation_results, save_path="results/model_comparison.png"):

        models = list(evaluation_results.keys())
        metrics = ['avg_reward', 'collision_rate', 'avg_speed', 'avg_lane_changes']
        metric_names = ['Average Reward', 'Collision Rate', 'Average Speed', 'Lane Changes per Episode']
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        axes = axes.flatten()
        
        colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']
        
        for i, (metric, name) in enumerate(zip(metrics, metric_names)):
            values = [evaluation_results[model][metric] for model in models]
            bars = axes[i].bar(models, values, alpha=0.8, color=colors[:len(models)])
            axes[i].set_title(name, fontsize=12, fontweight='bold')
            axes[i].set_ylabel(name)
            axes[i].tick_params(axis='x', rotation=45)

            for bar, value in zip(bars, values):
                height = bar.get_height()
                axes[i].text(bar.get_x() + bar.get_width()/2, height + max(values)*0.01,
                           f'{value:.3f}', ha='center', va='bottom', fontweight='bold')
            
            axes[i].grid(True, alpha=0.3)
        
        plt.tight_layout()
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"Result plot saved {save_path}")


def main():
    """‰∏ªÁ®ãÂ∫èÊµÅÁ®ã"""
    print("üöó Highway Decision Transformer ‰∏ªÁ®ãÂ∫èÂºÄÂßã")
    print("="*60)
    
    # ËÆæÂÆöÂèÇÊï∞
    styles = ["aggressive", "conservative", "normal"]
    ppo_models = {}
    trajectories = {}
    env_configs = {}
    evaluation_results = {}
    train_losses = []
    val_losses = []

    try:
        # === PPO ===
        print("\n Stage1: Training PPO model")
        print("-" * 30)
        
        for i, style in enumerate(styles, 1):
            print(f"\n[{i}/3] Training {style} PPO model...")
            trainer = HighwayPPOTrainer(style=style)
            env_configs[style] = trainer.env_config
            
            model = trainer.train_ppo(
                total_timesteps=20000,  
                save_path=f"models/ppo_{style}.zip"
            )
            ppo_models[style] = model
            print(f"{style} PPO modelTraining done")

        # === Stage 2 ÔºöCollect trajectory ===
        print("\nStage2: Collect trajectory")
        print("-" * 30)
        
        for i, style in enumerate(styles, 1):

            trainer = HighwayPPOTrainer(style=style)
            
            trajs = trainer.collect_trajectories(
                ppo_models[style], 
                num_episodes=15, 
                save_path=f"data/trajectories_{style}.pkl"
            )
            trajectories[style] = trajs

        # === Stage 3: Prepare data ===
        print("-" * 30)
        
        all_trajs = []
        for style in styles:
            all_trajs.extend(trajectories[style])

        if len(all_trajs) < 5:
            return

        context_length = 15  
        dataset = HighwayTrajectoryDataset(all_trajs, context_length=context_length)
        
        if len(dataset) == 0:
            return
            
        dataloader = DataLoader(dataset, batch_size=8, shuffle=True)

        # === Stage 4ÔºöDecision Transformer Training  ===
        print("\nStage4: Training Decision Transformer")
        print("-" * 30)
        
        sample_batch = next(iter(dataloader))
        sample_state = sample_batch['states'][0][0]
        state_dim = sample_state.shape[0]
        action_dim = 5

        dt_model = DecisionTransformer(
            state_dim=state_dim, 
            action_dim=action_dim, 
            context_length=context_length,
            embed_dim=64,   
            n_layer=2,      
            n_head=1,       
            dropout=0.1
        )
        
        dt_trainer = DecisionTransformerTrainer(dt_model, device, lr=1e-4)

        print("Training Decision Transformer...")
        try:
            train_losses, val_losses = dt_trainer.train(
                dataloader, 
                epochs=30  
            )
            
            os.makedirs("models", exist_ok=True)
            torch.save(dt_model.state_dict(), 'models/decision_transformer.pth')
            
        except Exception as e:
            print(f"Decision Transformer Training Error: {e}")
            train_losses = [2.0, 1.5, 1.2, 1.0]  
            val_losses = []

        # === Stage 5: model evaluation ===
        print("-" * 30)
        
        # Evaluate PPO model
        for i, style in enumerate(styles, 1):
            print(f"\n[{i}/4] Evaluate PPO_{style} model...")
            evaluator = HighwayEvaluator(env_configs[style])
            try:
                results, _ = evaluator.evaluate_model(
                    ppo_models[style], 
                    is_dt=False, 
                    num_episodes=10,  # Evaluate episodeÊï∞
                    target_return=10.0
                )
                evaluation_results[f"PPO_{style}"] = results
            
            except Exception as e:
                evaluation_results[f"PPO_{style}"] = {
                    'avg_reward': 0.0, 'std_reward': 0.0, 'avg_episode_length': 0.0,
                    'collision_rate': 1.0, 'avg_speed': 0.0, 'avg_lane_changes': 0.0
                }
        
        # Evaluate Decision Transformer
        print(f"\n[4/4] Evaluate Decision Transformer...")
        evaluator = HighwayEvaluator(env_configs["normal"])
        try:
            results, _ = evaluator.evaluate_model(
                dt_model, 
                is_dt=True, 
                num_episodes=10,
                target_return=10.0
            )
            evaluation_results["DecisionTransformer"] = results
        except Exception as e:
            evaluation_results["DecisionTransformer"] = {
                'avg_reward': 0.0, 'std_reward': 0.0, 'avg_episode_length': 0.0,
                'collision_rate': 1.0, 'avg_speed': 0.0, 'avg_lane_changes': 0.0
            }

        # === Stage 6: Visualize ===
        print("\n Stage6: Visualize")
        print("-" * 30)
        
        try:

            os.makedirs("results", exist_ok=True)
            os.makedirs("models", exist_ok=True)
            os.makedirs("data", exist_ok=True)
            
            models_dict = {
                "PPO_aggressive": ppo_models["aggressive"], 
                "PPO_conservative": ppo_models["conservative"], 
                "PPO_normal": ppo_models["normal"], 
                "DecisionTransformer": dt_model
            }
            
            visualizer = HighwayVisualizer(models_dict, env_configs["normal"])

            visualizer.plot_training_curves(train_losses, val_losses)
            visualizer.plot_evaluation_comparison(evaluation_results)
            
        except Exception as e:
            print(f"Error: {e}")

        
        sorted_results = sorted(evaluation_results.items(), 
                              key=lambda x: x[1]['avg_reward'], reverse=True)
        
        print("-" * 60)
        
        for i, (model_name, results) in enumerate(sorted_results, 1):
            print(f"{i:<4} {model_name:<20} {results['avg_reward']:<10.3f} "
                  f"{results['collision_rate']:<10.3f} {results['avg_speed']:<10.2f}")
        
        return evaluation_results, train_losses, val_losses
        
    except Exception as e:
        import traceback
        traceback.print_exc()
        return {}, [], []

if __name__ == "__main__":
    main()